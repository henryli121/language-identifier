{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb26c776-9431-4c40-9791-4926c2704a0a",
   "metadata": {},
   "source": [
    "# Group all top six language books #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "87780e66-4dd9-4134-a268-8f90ed01872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "dic = json.load(open(\"/usr/local/NNShared/hali/data/cleanBL.json\", 'r'))\n",
    "\n",
    "en = {}\n",
    "fr = {}\n",
    "fi = {}\n",
    "nl = {}\n",
    "de = {}\n",
    "pt = {}\n",
    "\n",
    "for books, languages in dic.items():\n",
    "    if languages.lower() == 'english':\n",
    "        en[books] = languages\n",
    "    elif languages.lower() == 'englishs':\n",
    "        en[books] = 'english'\n",
    "    elif languages.lower() == 'en':\n",
    "        en[books] = 'english'\n",
    "    elif languages.lower() == 'french':\n",
    "        fr[books] = languages\n",
    "    elif languages.lower() == 'finnish':\n",
    "        fi[books] = languages\n",
    "    elif languages.lower() == 'dutch':\n",
    "        nl[books] = languages\n",
    "    elif languages.lower() == 'german':\n",
    "        de[books] = languages\n",
    "    elif languages.lower() == 'portuguese': \n",
    "        pt[books] = languages\n",
    "    elif languages.lower() == 'portugese':\n",
    "        pt[books] = 'portuguese'\n",
    "        \n",
    "topsix = {\n",
    "    'en' : en,\n",
    "    'fr' : fr,\n",
    "    'fi' : fi,\n",
    "    'nl' : nl,\n",
    "    'de' : de,\n",
    "    'pt' : pt\n",
    "}\n",
    "\n",
    "print(len(topsix))\n",
    "\n",
    "with open(\"/usr/local/NNShared/hali/data/topsix.json\", 'w') as f:\n",
    "    json.dump(topsix, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ee553-566a-4259-9a4c-216928916554",
   "metadata": {},
   "source": [
    "# Define the function for getting characters from En books #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f502a9c4-c8cd-4be2-886a-a801804aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import json\n",
    "\n",
    "### load the json dictionary\n",
    "dic = json.load(open(\"/usr/local/NNShared/hali/data/topsix.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a53e075-4852-4f11-8d9e-2c53416ab9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encharacter():\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    ### grab the books and labels\n",
    "    for books, languages in dic['en'].items():\n",
    "        labels.append(languages)\n",
    "        with open(\"/usr/local/NNShared/hali/newebooks/\"+books+\".txt\",\"r\") as f:\n",
    "            maxlen = len(f.read())\n",
    "            f.seek(0) ### reset f.read() ##\n",
    "            \n",
    "            randpt = randrange(round((maxlen-30)/3), maxlen-30) ### grab the characters, starting from 1/3 position to avoid preface ###\n",
    "            char = f.read()[randpt:randpt+30]\n",
    "            texts.append(char)\n",
    "            f.close()\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b22e3-1299-40bf-ba16-f4ca899a3429",
   "metadata": {},
   "source": [
    "# Define the function for getting characters from non-En books #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282109c6-4157-4650-b285-561bb7fb548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nonencharacter(non_en_dic):\n",
    "    non_en_texts = []\n",
    "    non_en_labels = []\n",
    "    n = int(len(dic['en'])/len(non_en_dic))\n",
    "    \n",
    "    ### grab the books and labels\n",
    "    for books, languages in non_en_dic.items():\n",
    "        with open(\"/usr/local/NNShared/hali/newebooks/\"+books+\".txt\",\"r\") as f:\n",
    "            maxlen = len(f.read())\n",
    "            f.seek(0)   \n",
    "        \n",
    "            for i in range(n): ### grab 30 character from n random sequences from the book ###\n",
    "                randpt = randrange(round((maxlen-30)/3), maxlen-30) ### grab the characters, starting from 1/3 position to avoid preface ###\n",
    "                char = f.read()[randpt:randpt+30]\n",
    "                non_en_texts.append(char)\n",
    "                non_en_labels.append(languages)\n",
    "            \n",
    "                f.seek(0)  \n",
    "            \n",
    "            f.close()\n",
    "    return non_en_texts, non_en_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d76e88-398f-4010-998f-898cf57520da",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = Encharacter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97798ca0-dd9b-4617-ab82-6026251562a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_en_texts1, non_en_labels1 = Nonencharacter(dic['fr'])\n",
    "non_en_texts2, non_en_labels2 = Nonencharacter(dic['fi'])\n",
    "non_en_texts3, non_en_labels3 = Nonencharacter(dic['nl'])\n",
    "non_en_texts4, non_en_labels4 = Nonencharacter(dic['de'])\n",
    "non_en_texts5, non_en_labels5 = Nonencharacter(dic['pt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001ff48-2d70-440a-9c76-d784fc76dc39",
   "metadata": {},
   "source": [
    "#  Vectorize each character in each book #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd52ebd-2ecc-4672-b5e3-bf17a3e5f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d4ead9-6742-4523-9d6b-801a0ea9dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_vectorizer(strng, dic):\n",
    "    vector = [[0 if char != letter.lower() else 1 for char in dic] \n",
    "                for letter in strng]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97234927-5763-4dc2-b948-6fbe8dc2b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chartovec(characters):\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(characters)\n",
    "    sequences = tokenizer.texts_to_sequences(characters)\n",
    "    one_hot_results = tokenizer.texts_to_matrix(characters, mode='binary')\n",
    "    dic = tokenizer.word_index\n",
    "    \n",
    "    x_input = []\n",
    "    \n",
    "    for i in range(len(characters)):\n",
    "    \n",
    "        x_input.append(np.reshape(string_vectorizer(characters[i], dic),(30,len(dic)))) \n",
    "    \n",
    "    return np.array(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c0adf-fa96-4b7c-965b-e7e80a6f3483",
   "metadata": {},
   "source": [
    "# Vectorize the labels #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7bd1e42-d445-40f0-9e07-0cf7a9ca6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordtovec(tags):\n",
    "    tokenizer1 = Tokenizer(num_words = 1000)\n",
    "    tokenizer1.fit_on_texts(tags)\n",
    "    sequences1 = tokenizer1.texts_to_sequences(tags)\n",
    "    one_hot_results1 = tokenizer1.texts_to_matrix(tags, mode='binary')\n",
    "    dic1 = tokenizer1.word_index\n",
    "    y_input = np.array(string_vectorizer(tags, dic1))\n",
    "    \n",
    "    return y_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4e550-3f1a-4993-ade1-76603d8a7c03",
   "metadata": {},
   "source": [
    "# results #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0d5565-1333-4e32-ad8b-7a04f57edbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = texts + non_en_texts1 + non_en_texts2 + non_en_texts3 + non_en_texts4 + non_en_texts5\n",
    "tags = labels + non_en_labels1 + non_en_labels2 + non_en_labels3 + non_en_labels4 + non_en_labels5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3acb6953-bf26-4c2f-8f04-8a3cd7643744",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = chartovec(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2338914-d8ab-46a3-ad67-cc8d0d6e2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 'a': 3, 'n': 4, 'i': 5, 't': 6, 's': 7, 'o': 8, 'r': 9, 'l': 10, 'd': 11, 'u': 12, 'h': 13, 'm': 14, 'c': 15, ',': 16, 'p': 17, 'g': 18, 'k': 19, 'v': 20, '.': 21, 'b': 22, 'f': 23, 'ä': 24, 'w': 25, 'j': 26, 'y': 27, 'z': 28, '-': 29, 'q': 30, \"'\": 31, 'é': 32, '\"': 33, '_': 34, 'x': 35, ';': 36, '!': 37, 'ã': 38, 'ö': 39, 'ü': 40, ':': 41, 'ç': 42, '1': 43, '?': 44, 'à': 45, 'ß': 46, 'è': 47, 'á': 48, ')': 49, '2': 50, '0': 51, '(': 52, 'ê': 53, '»': 54, '«': 55, '3': 56, '8': 57, '5': 58, '4': 59, '*': 60, ']': 61, '[': 62, '6': 63, '7': 64, 'ó': 65, '9': 66, 'õ': 67, '|': 68, 'ô': 69, '=': 70, 'â': 71, 'î': 72, 'ù': 73, 'û': 74, '\\xa0': 75, '~': 76, '+': 77, 'ë': 78, 'í': 79, '&': 80, 'ï': 81, '/': 82, '©': 83, '^': 84, '{': 85, '#': 86, '}': 87, 'ú': 88, 'º': 89, '·': 90, '$': 91, 'ª': 92, 'æ': 93, '\\x92': 94, '§': 95, '°': 96, '>': 97, '<': 98, 'ò': 99, '¨': 100, 'å': 101, '\\t': 102, '´': 103, '¤': 104, 'ñ': 105, '%': 106, '¡': 107, 'ì': 108, '`': 109, 'þ': 110, '×': 111, '£': 112, '®': 113, '¶': 114, '\\\\': 115, '¼': 116, '\\x9f': 117, '\\x80': 118, '½': 119, '¿': 120, '¹': 121, '¢': 122, '\\x89': 123, '\\x97': 124, '\\x9d': 125, '\\x9c': 126, '²': 127, '\\x9b': 128, '\\x99': 129, '\\x94': 130, 'ð': 131, 'ø': 132, '\\x91': 133, 'ý': 134, '\\x82': 135, '\\x84': 136, '\\xad': 137}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(characters)\n",
    "sequences = tokenizer.texts_to_sequences(characters)\n",
    "one_hot_results = tokenizer.texts_to_matrix(characters, mode='binary')\n",
    "dic = tokenizer.word_index\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc1493a-2b92-4778-94fb-1d7c5fd18cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/usr/local/NNShared/hali/data/characterdic.json\", 'w') as f:\n",
    "    json.dump(dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69afc317-8121-4e28-b43a-14856e35b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_input = wordtovec(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23a89393-9efa-4f52-aa16-3fe29e057018",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = Tokenizer(num_words = 1000)\n",
    "tokenizer1.fit_on_texts(tags)\n",
    "sequences1 = tokenizer1.texts_to_sequences(tags)\n",
    "one_hot_results1 = tokenizer1.texts_to_matrix(tags, mode='binary')\n",
    "dic1 = tokenizer1.word_index\n",
    "\n",
    "with open(\"/usr/local/NNShared/hali/data/tagdic.json\", 'w') as f:\n",
    "    json.dump(dic1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dde0e87b-50bb-41b6-bea9-a2c803b54009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_input, Y_input = shuffle(X_input, Y_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86bdd52e-6684-47c3-845b-2b7f807bc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data \n",
    "with open(\"/usr/local/NNShared/hali/data/input_X.npy\", \"wb\") as f:\n",
    "    np.save(f, X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d20b752c-7e95-4bc2-963e-b7446bb2e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data \n",
    "with open(\"/usr/local/NNShared/hali/data/input_Y.npy\", \"wb\") as f:\n",
    "    np.save(f, Y_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7e962-b7ee-49cd-980c-731ec5eba9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
