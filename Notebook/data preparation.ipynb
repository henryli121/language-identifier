{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aca0f68-5351-433d-a1c2-649f8edb03ee",
   "metadata": {},
   "source": [
    "# Create a New Folder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721666d9-db37-4002-a968-adfb2b3c6b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/NNShared/hali'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbb907fc-7807-4022-8b32-473bc8a6cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/usr/local/NNShared/\")\n",
    "os.listdir()\n",
    "\n",
    "newpath = \"/usr/local/NNShared/hali/exceptions\" \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "path = \"/usr/local/NNShared/hali/ebooks\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e62e4-bd9a-4731-962f-c493fbd0ddd2",
   "metadata": {},
   "source": [
    "# Now Moving the Txt File #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daed0136-ecbc-4244-ac81-5386a61f90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/usr/local/NNShared/hali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7b4eef-a525-486f-8683-b456acf54870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = \"/usr/local/NNShared/hali/exceptions\"\n",
    "for f in os.listdir(dir):\n",
    "    os.remove(os.path.join(dir, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca843e4-de5c-40bc-b661-e0934ac3e638",
   "metadata": {},
   "source": [
    "Check for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6e02ab87-a3a9-41b6-970c-eb638dffc5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "36100\n",
      "43\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/usr/local/NNShared/hali\")\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali\")))\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali/ebooks\")))\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali/exceptions\")))\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali/data\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122df6f-f9cd-4176-be0e-67fdf8f1e1ae",
   "metadata": {},
   "source": [
    "# Find the language tag for each book #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0d4a5271-f3d3-4a86-99b2-579b4f2e473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36099\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "languages = []\n",
    "\n",
    "for file in os.listdir(\"/usr/local/NNShared/hali/ebooks\"):\n",
    "    thisfile = open(\"/usr/local/NNShared/hali/ebooks/\"+file, \"r\", encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    for x in thisfile:\n",
    "        y = re.search('Language:', x)\n",
    "        if y!= None:\n",
    "            languages.append(y.string.rstrip().split(\": \")[1])\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 0:\n",
    "        shutil.copy(\"/usr/local/NNShared/hali/ebooks/\"+file, \"/usr/local/NNShared/hali/exceptions\")\n",
    "        os.remove(\"/usr/local/NNShared/hali/ebooks/\"+file)\n",
    "\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali/ebooks\")))\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali/exceptions\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab3ccab1-c2ee-4c60-8634-24f151df2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36099\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.chdir(\"/usr/local/NNShared/hali/ebooks\")\n",
    "\n",
    "books = []\n",
    "\n",
    "for file in os.listdir(\"/usr/local/NNShared/hali/ebooks\"):\n",
    "    if file == '20060205-871-8.txt':\n",
    "        books.append('20060205-871')\n",
    "    elif file == '20100823-8387-8.txt':\n",
    "        books.append('20100823-8387')\n",
    "    elif file == '20080731-834-8.txt':\n",
    "        books.append('20080731-834')\n",
    "    else:\n",
    "        books.append(file.split(\"-8\")[0])\n",
    "        \n",
    "    \n",
    "print(len(books))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ff032-ee36-4a1d-b9e9-6238b45e2682",
   "metadata": {},
   "source": [
    "# Save them as a json file #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6edf022b-4dfc-47be-a609-424bae354206",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookandlang = list(zip(books, languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "59e5043b-d37b-4360-8b5e-472785dd6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "newpath = \"/usr/local/NNShared/hali/data\" \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "with open(\"/usr/local/NNShared/hali/data/data.json\", 'w') as f:\n",
    "    json.dump(bookandlang, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9843cea6-39bd-4ed4-b712-07da1fcc1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36099\n"
     ]
    }
   ],
   "source": [
    "#read the json file#\n",
    "\n",
    "import json\n",
    "\n",
    "json_file_path = \"/usr/local/NNShared/hali/data/data.json\"\n",
    "\n",
    "with open(json_file_path, 'r') as j:\n",
    "     contents = json.loads(j.read())\n",
    "\n",
    "print(len(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf0a3d-ac9b-430f-9b49-95b13d27f4d9",
   "metadata": {},
   "source": [
    "# Remove the header and footer of each book #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7cdea9-cd81-4b6b-a34f-910d606594d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36099\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "newpath = \"/usr/local/NNShared/hali/newebooks\" \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "for pairs in contents:\n",
    "    f = open(\"/usr/local/NNShared/hali/ebooks/\"+pairs[0]+\"-8.txt\", \"r\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "    flag = 0\n",
    "\n",
    "    sample = []\n",
    "\n",
    "    for myline in f:\n",
    "        y = re.search(\"\\\\*\\\\*\\\\* START OF\", myline)\n",
    "        z = re.search(\"\\\\*\\\\*\\\\* END OF\",myline)\n",
    "        a = re.search(\"\\\\*END THE SMALL PRINT\", myline)\n",
    "        b = re.search(\"End of the Project\", myline)\n",
    "        p = re.search(\"\\\\*\\\\*\\\\*START OF\", myline)\n",
    "        q = re.search(\"\\\\*\\\\*\\\\*END OF\", myline)\n",
    "\n",
    "        if z:\n",
    "            flag = 0\n",
    "        elif b:\n",
    "            flag = 0\n",
    "        elif q:\n",
    "            flag = 0\n",
    "            \n",
    "        if flag == 1:\n",
    "            if myline != '\\n':\n",
    "                sample.append(myline.rstrip())\n",
    "        if y:\n",
    "            flag = 1\n",
    "        elif a:\n",
    "            flag = 1\n",
    "        elif p:\n",
    "            flag = 1\n",
    "    \n",
    "    with open(\"/usr/local/NNShared/hali/newebooks/\"+pairs[0]+\".txt\", 'w') as file:\n",
    "        file.write(' '.join(sample))\n",
    "\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali/newebooks\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d5989-79d8-4290-b5c7-4634d79a2490",
   "metadata": {},
   "source": [
    "# Remove those books which does not have standard header and footer format #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a8e1e4-3a68-4479-ae89-4babe6e467c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36082\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for files in os.listdir(\"/usr/local/NNShared/hali/newebooks\"):\n",
    "    books = open(\"/usr/local/NNShared/hali/newebooks/\"+files,\"r\")\n",
    "    #open each new book#\n",
    "        \n",
    "    length = len(books.read())\n",
    "        \n",
    "    if length < 150:\n",
    "        os.remove(\"/usr/local/NNShared/hali/newebooks/\"+files)\n",
    "\n",
    "print(len(os.listdir(\"/usr/local/NNShared/hali/newebooks\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b46f6-8a93-49e1-9ba9-36c5f7aa6925",
   "metadata": {},
   "source": [
    "# Match the left books with their language tag and save as a dictionary #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1468add4-2a64-4ad5-a756-cf2692043fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36082\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "json_file_path = \"/usr/local/NNShared/hali/data/data.json\"\n",
    "\n",
    "with open(json_file_path, 'r') as j:\n",
    "        contents = json.loads(j.read())\n",
    "\n",
    "bookswithlang = {}\n",
    "        \n",
    "for newbooks in os.listdir(\"/usr/local/NNShared/hali/newebooks\"):\n",
    "    for books in contents:\n",
    "        if newbooks.split(\".t\")[0] == books[0]:\n",
    "            bookswithlang[newbooks.split(\".t\")[0]] = books[1]\n",
    "\n",
    "print(len(bookswithlang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5659819-9bd3-4bf9-834c-f8558c4cc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the number of books matches #\n",
    "\n",
    "a = []\n",
    "\n",
    "for newbooks in os.listdir(\"/usr/local/NNShared/hali/newebooks\"):\n",
    "    flag = 0\n",
    "    for books in contents:\n",
    "        if newbooks.split(\".t\")[0] != books[0]:\n",
    "            flag = flag + 1\n",
    "    if flag == 36099:\n",
    "        print(newbooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db484da2-7bc5-417d-be1c-b4972af526bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data.json', 'cleanBL.json']\n"
     ]
    }
   ],
   "source": [
    "#check the json files #\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"/usr/local/NNShared/hali/data/cleanBL.json\", 'w') as f:\n",
    "    json.dump(bookswithlang, f)\n",
    "\n",
    "print(os.listdir(\"/usr/local/NNShared/hali/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b60db2b-e5ce-4d97-8545-a7975382b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read our dictionary#\n",
    "import json\n",
    "f = open(\"/usr/local/NNShared/hali/data/cleanBL.json\", 'r')\n",
    "\n",
    "dic = json.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef34741-ad43-4f25-b639-852f89144dac",
   "metadata": {},
   "source": [
    "# Vectorize each character in each book #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ad255e-51b8-4733-aa62-a138cb8c45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "dictionary = json.load(open(\"/usr/local/NNShared/hali/data/cleanBL.json\", 'r'))\n",
    "\n",
    "for books, languages in dictionary.items():\n",
    "    labels.append(languages)\n",
    "    with open(\"/usr/local/NNShared/hali/newebooks/\"+books+\".txt\",\"r\") as f:\n",
    "        text = f.read()[50:200]\n",
    "        texts.append(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192e4c3d-4be7-43cc-90ea-18839e3af8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 't': 3, 'a': 4, 'i': 5, 'r': 6, 'n': 7, 'o': 8, 's': 9, 'd': 10, 'l': 11, 'p': 12, 'h': 13, 'm': 14, 'g': 15, 'u': 16, 'w': 17, 'b': 18, '.': 19, 'c': 20, 'f': 21, 'y': 22, '/': 23, 'v': 24, '-': 25, ':': 26, 'k': 27, ',': 28, 'j': 29, '(': 30, '1': 31, ')': 32, '_': 33, 'ä': 34, \"'\": 35, '\"': 36, '9': 37, '8': 38, '[': 39, '*': 40, 'x': 41, '0': 42, ']': 43, '2': 44, 'z': 45, 'q': 46, '=': 47, 'ö': 48, '3': 49, '7': 50, '5': 51, 'é': 52, '4': 53, '6': 54, 'è': 55, '&': 56, '+': 57, ';': 58, '\\t': 59, 'ü': 60, '?': 61, '!': 62, '#': 63, '|': 64, 'ó': 65, 'ã': 66, 'ç': 67, 'á': 68, 'å': 69, '@': 70, 'í': 71, 'à': 72, '·': 73, 'æ': 74, 'ñ': 75, '»': 76, 'º': 77, '{': 78, 'ê': 79, 'â': 80, 'ë': 81, '«': 82, '}': 83, '<': 84, '¡': 85, 'ð': 86, '°': 87, '¤': 88, '~': 89, 'ò': 90, 'ß': 91, '©': 92, '>': 93, 'î': 94, 'ô': 95, 'õ': 96, 'ï': 97, '\\xa0': 98, '^': 99, '¨': 100, 'ø': 101, 'û': 102, 'ú': 103, '\\x92': 104, '¥': 105, '$': 106, '\\x80': 107, 'ì': 108, 'ª': 109, '§': 110, 'ý': 111, '½': 112, '`': 113, 'ù': 114, '¦': 115, '£': 116, '¼': 117, '\\x93': 118, '×': 119, '\\x89': 120, '\\x94': 121, '¢': 122, '\\\\': 123, 'ÿ': 124, '±': 125, '¹': 126, '¸': 127, '¾': 128, '÷': 129, '¶': 130, '®': 131, '\\xad': 132, '%': 133, '\\x9c': 134, '\\x9d': 135, '\\x95': 136, '¿': 137, 'þ': 138, '\\x99': 139, '²': 140, '\\x8c': 141, '\\x8a': 142, '³': 143, '\\x97': 144, '\\x98': 145, 'µ': 146, '\\x8e': 147, '\\x88': 148, '\\x96': 149, '\\x9b': 150, '\\x82': 151, '\\x85': 152, '¬': 153, '¯': 154, '´': 155, '\\x83': 156, '\\x91': 157, '\\x90': 158, '\\x8f': 159, '\\x9e': 160}\n",
      "36082\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 1 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 1 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 1 ... 0 0 0]\n",
      "  ...\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 1 ... 0 0 0]\n",
      "  [0 1 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 1 ... 0 0 0]\n",
      "  [0 1 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 1 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 1 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "\n",
    "dic = tokenizer.word_index\n",
    "\n",
    "def string_vectorizer(strng, dic):\n",
    "    vector = [[0 if char != letter.lower() else 1 for char in dic] \n",
    "                  for letter in strng]\n",
    "    return vector\n",
    " \n",
    "lst = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    \n",
    "    lst.append(np.reshape(string_vectorizer(texts[i], dic),(150,len(dic)))) \n",
    "    \n",
    "print(len(np.array(lst)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0868a33d-4dc2-491e-ae4c-c21481898d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36082, 150, 160)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(lst).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0230495f-1ee4-41c7-8817-32efa064c4ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-15c72849deef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lst' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.array(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0001e7f6-c19f-4913-afb9-8adb1bcddf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data \n",
    "with open(\"/usr/local/NNShared/hali/data/input_X.npy\", \"wb\") as f:\n",
    "    np.save(f, np.array(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d854e-96fd-4b29-824d-a2afde5cf1a0",
   "metadata": {},
   "source": [
    "# Vectorize our labels #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c36bce1-2e6f-480a-a974-ef4b22988edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 1, 'french': 2, 'finnish': 3, 'german': 4, 'dutch': 5, 'portuguese': 6, 'italian': 7, 'spanish': 8, 'swedish': 9, 'danish': 10, 'latin': 11, 'tagalog': 12, 'and': 13, 'catalan': 14, 'hungarian': 15, 'norwegian': 16, 'esperanto': 17, 'en': 18, 'greek': 19, 'afrikaans': 20, 'with': 21, 'icelandic': 22, 'polish': 23, 'friulian': 24, 'chinese': 25, 'friulan': 26, 'nu': 27, 'cebuano': 28, 'frisian': 29, 'gaelic': 30, 'nahuatl': 31, 'russian': 32, 'serbian': 33, 'flemish': 34, 'iloko': 35, 'czech': 36, 'japanese': 37, 'comments': 38, 'character': 39, 'set': 40, 'encoding': 41, 'estonian': 42, 'multiple': 43, 'inuktitut': 44, 'bagobo': 45, 'kashubian': 46, 'arapaho': 47, 'welsh': 48, 'galician': 49, 'breton': 50, 'careful': 51, 'bulgarian': 52, 'venetian': 53, 'neapolitan': 54, 'gascon': 55, 'ancient': 56, 'portugese': 57, 'old': 58, 'middle': 59, '1100': 60, '1500': 61, 'nederlands': 62, 'ilocano': 63, 'chinook': 64, 'quiche': 65, 'onondaga': 66, 'khasi': 67, 'language': 68, 'spoken': 69, 'in': 70, 'n': 71, 'e': 72, 'india': 73, 'irish': 74, 'scots': 75, 'englishs': 76, 'slovenian': 77, 'romanian': 78, 'this': 79, 'is': 80, 'the': 81, '8': 82, 'bit': 83, 'unaccented': 84, 'version': 85}\n",
      "(36082, 85)\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = Tokenizer(num_words = 1000)\n",
    "\n",
    "tokenizer1.fit_on_texts(labels)\n",
    "\n",
    "sequences1 = tokenizer1.texts_to_sequences(labels)\n",
    "\n",
    "one_hot_results1 = tokenizer1.texts_to_matrix(labels, mode='binary')\n",
    "\n",
    "dic1 = tokenizer1.word_index\n",
    "\n",
    "ans = np.array(string_vectorizer(labels, dic1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba83b966-1286-4c61-932a-01be4b37fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data \n",
    "with open(\"/usr/local/NNShared/hali/data/input_Y.npy\", \"wb\") as f:\n",
    "    np.save(f, ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
